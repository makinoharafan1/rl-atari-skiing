# RL Atari Skiing

## Описание

Вы управляете лыжником, который может двигаться боком. Цель состоит в том, чтобы пробежать все ворота (между столбами) за максимально короткое время. За каждый пропущенный проход вы получаете штраф в размере пяти секунд. Если вы врежетесь в ворота или дерево, ваш лыжник вскочит на ноги и продолжит движение.

## Действия

Skiing имеет такое пространство действия, как Discrete(3): NOOP, LEFT, RIGHT. То есть лыжник может повернуть либо влево, либо вправо или не поворачивать.

## Награды

Секунды - это единственная награда. Отрицательные награды и штрафы (например, за пропущенные ворота) начисляются в качестве дополнительных секунд.

## Точечная оценка

В контексте обучения с подкреплением, точечная оценка \( J \) относится к оценке среднего значения функции вознаграждения (total reward) за определенный период времени или после выполнения определенного количества действий.

Пусть у нас есть последовательность действий, приводящих к наградам $r_1, r_2, \dots, r_n$, где $r_i$ - награда, полученная после выполнения $i$-го действия. Тогда точечная оценка $J$ для этой последовательности может быть вычислена как среднее значение наград:


$J = \frac{1}{n} \sum_{i=1}^{n} r_i$


То есть это просто среднее значение наград за период времени или за определенное количество действий.

## Интервальная оценка

Интервальная оценка $J$ в обучении с подкреплением используется для определения доверительного интервала, который показывает диапазон значений, в котором находится среднее значение функции вознаграждения (total reward) с определенной вероятностью.

После выполнения определенного количества эпизодов обучения и собирания наград, мы можем рассчитать точечную оценку ${J}$ для среднего значения наград. Далее, используя статистические методы, мы можем рассчитать доверительный интервал, который показывает, с какой вероятностью среднее значение наград находится в определенном диапазоне.

Один из наиболее распространенных способов рассчета доверительного интервала - это использование бутстрепа. В бутстрепе мы многократно выбираем случайные выборки из наших данных и повторяем оценку точечной оценки и расчет доверительного интервала на каждой выборке. Затем мы используем распределение полученных оценок, чтобы определить доверительный интервал.

Формально, пусть ${J}$ - это точечная оценка для среднего значения наград, а ${J_1}$, ${J_2}$, $\dots$, ${J_k}$ - это оценки, полученные из бутстреп-выборок. Тогда доверительный интервал с уровнем доверия 1 - $\alpha$ будет определен как интервал между ${J_{a/2}}$ и ${J_{1-a/2}}$, где $\alpha$ - это уровень значимости (обычно 0.05 для 95% доверительного интервала).

Интервальная оценка позволяет учитывать неопределенность и изменчивость в данных и предоставляет более информативную оценку среднего значения наград в обучении с подкреплением.


# Политики/алгоритмы, примененные для обучения 

## 1. Эпсилон-жадная стратегия (epsilon-greedy policy)

Жадный алгоритм можно определить как алгоритм, который выбирает наилучший доступный в настоящее время вариант без учета долгосрочных последствий этого решения, которое может оказаться неоптимальным. Учитывая это, мы можем определить эпсилон-жадный алгоритм как жадный алгоритм, который добавляет некоторую случайность при выборе между вариантами: вместо того, чтобы всегда выбирать лучший доступный вариант, случайным образом исследуйте другие варианты с вероятностью = $e$ или выбрать лучший вариант с вероятностью = 1 - $e$. Поэтому мы можем добавить в алгоритм случайности, увеличив e, что заставит алгоритм чаще исследовать другие варианты.

Изменение Q-значения в обновлении Q-функции используется TD (temporal difference) обучение, это обновление сочетает в себе методы TD и метод обучения с подкреплением, известный как Q-обучение.

$Q(a) \leftarrow Q(a) + \alpha \left( r + \gamma \max_{a'} Q(a') - Q(a) \right)$

Формула выше обновляет $Q(a)$ в направлении, которое учитывает текущую награду, максимальное ожидаемое будущее вознаграждение и текущую оценку $Q(a)$.

## Пример развернутой среды (в виде траектории), используя стратегию epsilon-greedy

![](assets/epsilon-greedy-episode5.gif)

## 2. SARSA (State-Action-Reward-State-Action) со стратегий ε-greedy

Алгоритм SARSA является одним из методов обучения с подкреплением. Он используется для обучения агентов в средах, где действия агента и их результаты зависят от текущего состояния среды. Основная идея заключается в обновлении оценок ценности действий на основе полученных наград и следующих состояний. Суть алгоритма в том, что агент оценивает ценность каждого действия в каждом состоянии и выбирает действие согласно определенной стратегии (например, ε-greedy), а затем обновляет свои оценки, основываясь на награде и следующем состоянии. Таким образом, SARSA обновляет оценки Q-значений (ценности действий в состоянии) на основе конкретных пар состояние-действие-награда-следующее состояние.

Выбор данного алгоритма обосновывается тем, что нам удалось существенно успростить представление состояния, а именно из RGB массива-картинки перевести его в пару из двух чисел: положение лыжника по оси x и положение по оси x центра между флагами.

## 3. Actor-Critic

Конечная цель алгоритма Actor-Critic - обучить агента, принимающего решения в среде, где целью является максимизация общей награды. Он сочетает в себе две основные идеи:

Актер (Actor):

Актер отвечает за выбор действий в среде. Его цель - научиться выбирать оптимальные действия, чтобы максимизировать награду.
Актер представлен в виде политики, которая отображает состояния среды в вероятности выполнения действий.
Обучение актера основано на методах градиентного подъема, где градиенты вычисляются для максимизации ожидаемой награды.

Критик (Critic):

Критик оценивает состояния среды, предоставляя оценку их ценности или ожидаемой награды.
Его цель - научиться точно оценивать, насколько хороши или плохи текущие состояния среды для агента.
Критик обучается методами обучения с учителем на основе полученных наград и оценок состояний.
В алгоритме Actor-Critic эти две компоненты работают вместе и обучаются параллельно. Актер использует оценки критика для вычисления градиента, который указывает, как изменить свою политику действий, чтобы увеличить ожидаемую награду. Критик, в свою очередь, использует актера для генерации образцов состояний, на которых он обучается.

Таким образом, Actor-Critic является гибридным методом, который комбинирует преимущества обучения с подкреплением и методов оценки ценности, позволяя агенту эффективно изучать и адаптироваться к сложным средам.
