{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N экспериментов?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для расчета интервальной оценки используем стандартное отклонение (`σ`).\n",
    "\n",
    "**Уровень значимости α = 0.1:**\n",
    "\n",
    "\n",
    "\n",
    "$ n = \\left(\\frac{{z \\cdot σ}}{{\\text{точность}}}\\right)^2 $\n",
    "\n",
    "**Уровень значимости α = 0.05:**\n",
    "\n",
    "\n",
    "$ n = \\left(\\frac{{z \\cdot σ}}{{\\text{точность}}}\\right)^2 $\n",
    "\n",
    "Здесь:\n",
    "- `n` - количество экспериментов,\n",
    "- `σ` - стандартное отклонение выборки,\n",
    "- `z` - z-значение для заданного уровня значимости,\n",
    "- `точность` - половина ширины доверительного интервала.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Теоретический анализ сложности алгоритмов SARSA и Actor-Critic\n",
    "\n",
    "Сравнение по длине эпизодов нецелесообразно, так как функция награды итак отражает время, затраченное на выполнение эпизода, поэтому проведем теоретический анализ:\n",
    "\n",
    "## 1. Вычислительная сложность\n",
    "\n",
    "### SARSA\n",
    "- **Основные операции**: Обновление Q-значений, выбор действий.\n",
    "- **Обновление Q-значений**: SARSA обновляет одно Q-значение на каждом шаге.\n",
    "  $\n",
    "  Q(s_t, a_t) \\leftarrow Q(s_t, a_t) + \\alpha [r_{t+1} + \\gamma Q(s_{t+1}, a_{t+1}) - Q(s_t, a_t)]\n",
    "  $\n",
    "- **Сложность на один шаг**: O(1), так как обновление одного Q-значения требует постоянного времени.\n",
    "- **Выбор действий**: O(A), где A - количество возможных действий (если используется ε-жадная стратегия).\n",
    "\n",
    "### Actor-Critic\n",
    "- **Основные операции**: Обновление функции ценности (Critic), обновление политики (Actor), выбор действий.\n",
    "- **Обновление функции ценности (Critic)**: \n",
    "  $\n",
    "  V(s_t) \\leftarrow V(s_t) + \\alpha [r_{t+1} + \\gamma V(s_{t+1}) - V(s_t)]\n",
    "  $\n",
    "- **Обновление политики (Actor)**: \n",
    "  $\n",
    "  \\theta \\leftarrow \\theta + \\beta \\nabla_\\theta \\log \\pi(a_t | s_t) [r_{t+1} + \\gamma V(s_{t+1}) - V(s_t)]\n",
    "  $\n",
    "- **Сложность на один шаг**:\n",
    "  - Обновление функции ценности: O(1)\n",
    "  - Обновление политики: зависит от сложности вычисления градиента, обычно O(d), где d - количество параметров политики.\n",
    "- **Выбор действий**: зависит от стратегии, обычно O(A) для дискретных действий.\n",
    "\n",
    "## 2. Требования к памяти\n",
    "\n",
    "### SARSA\n",
    "- **Хранение Q-таблицы**: Требуется память для хранения Q-значений для всех пар состояний и действий.\n",
    "  $\n",
    "  O(S \\times A)\n",
    "  $\n",
    "  где S - количество состояний, A - количество действий.\n",
    "\n",
    "### Actor-Critic\n",
    "- **Хранение параметров критика**: Память для хранения параметров функции ценности.\n",
    "  $\n",
    "  O(S)\n",
    "  $\n",
    "- **Хранение параметров актера**: Память для хранения параметров политики.\n",
    "  $\n",
    "  O(d)\n",
    "  $\n",
    "- **Общий объем памяти**: \n",
    "  $\n",
    "  O(S + d)\n",
    "  $\n",
    "  где d - количество параметров политики.\n",
    "\n",
    "## 3. Сходимость\n",
    "\n",
    "### SARSA\n",
    "- **Сходимость**: Гарантированная сходимость при соблюдении условий на α (скорость обучения) и γ (дисконт-фактор).\n",
    "- **Теоретическая стабильность**: Более устойчива к вариациям, так как является on-policy алгоритмом.\n",
    "\n",
    "### Actor-Critic\n",
    "- **Сходимость**: Меньше теоретических гарантий по сравнению с SARSA, но может сходиться быстрее в практике.\n",
    "- **Теоретическая стабильность**: Меньше устойчивости, так как используется градиентный метод для обновления политики (off-policy).\n",
    "\n",
    "## 4. Устойчивость\n",
    "\n",
    "### SARSA\n",
    "- **Устойчивость**: Высокая устойчивость благодаря on-policy подходу, что делает алгоритм менее подверженным нестабильности.\n",
    "\n",
    "### Actor-Critic\n",
    "- **Устойчивость**: Меньшая устойчивость, так как разделение оценки и политики может приводить к нестабильности, особенно при неправильной настройке гиперпараметров.\n",
    "\n",
    "## Заключение\n",
    "\n",
    "### SARSA\n",
    "- **Плюсы**:\n",
    "  - Простая реализация и теоретическое обоснование.\n",
    "  - Высокая устойчивость и гарантированная сходимость.\n",
    "  - Низкие требования к памяти и вычислительной сложности.\n",
    "- **Минусы**:\n",
    "  - Менее эффективен в больших и сложных пространствах состояний и действий.\n",
    "  - Ограниченная возможность оптимизации политики.\n",
    "\n",
    "### Actor-Critic\n",
    "- **Плюсы**:\n",
    "  - Высокая гибкость и масштабируемость для сложных задач с непрерывными действиями.\n",
    "  - Возможность быстрого обучения и эффективного обновления политики.\n",
    "- **Минусы**:\n",
    "  - Более сложная реализация и настройка.\n",
    "  - Более высокие требования к памяти и вычислительным ресурсам.\n",
    "  - Потенциальная нестабильность и меньше теоретических гарантий сходимости."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl-skiing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
